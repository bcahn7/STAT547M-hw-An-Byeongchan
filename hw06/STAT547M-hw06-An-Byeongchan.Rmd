---
title: "STAT547M-hw06-An-Byeongchan"
output: 
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Pick (at least) two of the six (numbered) topics below and do one of the exercise prompts listed, or something comparable using your dataset of choice.

 - I chose Q2, Q4, Q5. 

### 5. Work with a list
`purrr`'s map function is useful to get information out of a non-rectangular data structure.  
The `map` functions transform their input by applying a function to **each element** and returning a vector the same length as the input.  
- `map()`, `map2()`, `pmap()` return a list.
- `map_lgl()`, `map_int()`, `map_chr()` return vectors of the corresponding type.  


## Trump Android words
This is a text analysis of Trump's tweets. 
```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(purrr))


#Load some tweets from the official Donald Trump account.
load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))
tweets <- trump_tweets_df$text
tweets %>% head() %>% strtrim(70)

#Some words that were shown to be associated with Trump tweets.
regex <- "badly|crazy|weak|spent|strong|dumb|joke|guns|funny|dead"

#Select some tweets.
tweets <- tweets[c(1, 2, 5, 6, 198, 347, 919)]
tweets %>% strtrim(70)
```

With the selected tweets, I will investigate which words in the bag of words are mentioned in each tweet. `substring(text, first, last)` is one way to solve this. We need each words first position and last position. `gregexpr()` function can be used to extract the positions.  
`map` functions will be used to apply this to each tweet in a list.  

```{r}
#Extract the information of the words in each tweet. Output is a list
#First position and length!
matches <- gregexpr(regex, tweets)
matches
str(matches)
#View(matches)


## Exercise: Get a list of the match lengths. This is for the last position of each word. 
#Extracting attributes named `match.length()` from each element of the list `matches`
map(matches, function(x) attr(x, which = "match.length"))
#map(matches, ~ attr(.x, which = "match.length"))


## Exercise: Count the number of Trump Android words in each tweets.

# lengths() is for the length of each element of `matches`
#`map_int()` is another way to do it.
# BUT these cannot distinguish between 0 and 1.
lengths(matches)
map_int(matches, length)

# Thus, apply `map_int()` and modify it!!
map_int(matches, function(x) sum(x>0))


# Removing attr for less clutter
# Using `as.vector()` to strip attributes.
match_vec <- map(matches, as.vector)
match_vec

## Extracting exact words mentioned on the tweet
# What words are mentioned in tweet #7??
tweet_7 <- tweets[7]
match_first <- map(matches, as.vector)
match_length <- map(matches, function(x) attr(x, which = "match.length"))

#The first position of each word in tweet #7
(t_first <- match_first[[7]])
#The length of each word in tweet #7
(t_length <- match_length[[7]])
#Then, we can extract the last position of each word.
(t_last <- match_first[[7]] + match_length[[7]] - 1)
#The words mentioned in tweet #7
substring(tweet_7, t_first, t_last)


#Expand it to all the tweets.
#Extracting last positions.
match_last  <- map2(match_first, match_length, ~.x + .y -1) # 2 inputs

#Using `pmap()` we can extract the words mentioned in each tweet.
#`pmap()` takes a list as an input.
pmap(.l= list(text = tweets, first = match_first, last = match_last), substring) 

#another way to do this
mdf <- tibble(
  text = tweets,
  first = match_first,
  last = match_last
)
pmap(mdf, substring)

```


## Steps to extract the words in a text
Pseudo-code
> text <- an element of a text
> first <- an element of match_first
> last <- an element of match_last
> substring(text, first, last)


## Application to Finance
I tried to apply this skill to analyze the minutes of the FOMC (Federal Open Market Committee) meetings.   
  
I downloaded the data from https://stanford.edu/~rezab/useful/fomc_minutes.html 
  
Downloaded: 3 minutes (20010627, 20061025, 20071211)
Bag of words: inflation, investment, markets, economy, monetary, unemployment  

This is basic and introductory textual analysis to see if there is a change in the number of words mentioned during different periods (IT bubble, 2008 Financial Crisis)
```{r}
## Read the data and make them ready.
#m200106 <- scan("Data/20010627.txt", what="character", sep = NULL)
m200106 <- read_file("Data/20010627.txt")
m200610 <- read_file("Data/20061025.txt")
m200712 <- read_file("Data/20071211.txt")

#Bag of words
bow <- "inflation|investment|markets|economy|monetary|unemployment"
collection  <-  list(m200106, m200610, m200712)
str(collection)

#Extracting all the words mentioned in each minute. 
wrd_mtnd <- tibble(text = collection,
       first = gregexpr(bow, collection)) %>% 
  mutate (match_length = map(first, ~attr(.x, which = "match.length")),
          last = map2(first, match_length, ~ .x + .y -1)) %>% 
  select(-match_length) %>% 
  pmap(substring)
map(wrd_mtnd, ~table(.x)) 
```


### 2. Writing functions

I made a function `Reg_life_gdp(mode, year)` which can do a regression in a specific year with cross-sectional data (lifeExp vs. gdpPercap; Regress gdpPercap on lifeExp). There are three different modes; Linear model, Quadratic model, and Robust model.  


```{r}
suppressPackageStartupMessages(library(gapminder))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(MASS))

names(gapminder)
#Cross-sectional data (lifeExp vs gdpPercap) in 2007
gap_2007 <- gapminder %>% 
  filter(year==2007) 
gap_2007 %>% 
  ggplot(aes(x=lifeExp, y=gdpPercap)) +
  geom_point() +
  geom_smooth(method = "lm", se=FALSE, color = "blue") +
  geom_smooth(method = "lm", se=FALSE, formula = y~poly(x,2), color= "green") +
  geom_smooth(method = "rlm", se=FALSE, color= "red") +
  geom_smooth(method = "auto", se=FALSE, color = "purple")

lm(gdpPercap ~ lifeExp, gap_2007)
gap_2007_temp <- gap_2007 %>% 
  mutate(lifeExp2=  lifeExp^2)
lm(gdpPercap ~ lifeExp + lifeExp2, gap_2007_temp)
rlm(gdpPercap ~ lifeExp, gap_2007)

Reg_life_gdp<- function(mode, year){
  gap_year <- gapminder %>% 
    filter(year == year) %>% 
    mutate(lifeExp2 = lifeExp^2)
  if(mode=="lm"){
    the_fit <- lm(gdpPercap ~ lifeExp, gap_year)
    setNames(coef(the_fit), c("intercept", "slope"))
  }
  else if(mode=="qm"){
    the_fit <- lm(gdpPercap ~ lifeExp + lifeExp2, gap_year)
    setNames(coef(the_fit), c("intercept", "slope1", "slope2"))
  }
  else if(mode=="rlm"){
    the_fit <- rlm(gdpPercap ~ lifeExp, gap_year)
    setNames(coef(the_fit), c("intercept", "slope1"))    
  }
}

Reg_life_gdp("lm", 2002)
Reg_life_gdp("rlm", 2007)
```


### 4. Work with the `singer` data

The `singer_location` dataframe in the `singer` package contains geographical information stored in two different formats: 1. as a (dirty!) variable named `city`; 2. as a latitude / longitude pair (stored in `latitude`, `longitude` respectively). The function `revgeocode` from the `ggmap` library allows you to retrieve some information for a pair (vector) of longitude, latitude (warning: notice the order in which you need to pass lat and long). Read its manual page.
```{r}
suppressPackageStartupMessages(library(singer))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(tidyverse))
#install.packages('ggmap')
suppressPackageStartupMessages(library(ggmap))
#View(singer_locations)
```

1. Use `purrr` to map latitude and longitude into human readable information on the band's origin places. Notice that `revgeocode(... , output = "more")` outputs a dataframe, while `revgeocode(... , output = "address")` returns a string: you have the option of dealing with nested dataframes.  
You will need to pay attention to two things:  
    *  Not all of the track have a latitude and longitude: what can we do with the missing information? (_filtering_, ...)
    *  Not all of the time we make a research through `revgeocode()` we get a result. What can we do to avoid those errors to bite us? (look at _possibly()_ in `purrr`...)  
    
Human readable information on the band's origin places is extracted through `revgeocode()`. When `revgeocode()` is used, all the observations with NA should be removed. I used `filter()` to solve this.
```{r}
glimpse(singer_locations)

#It takes so long time that I just try it with some observations.
singer_loc <- singer_locations %>% 
  filter(!is.na(longitude|latitude)) %>% 
  head() %>% 
  mutate(loc = map2(longitude, latitude, ~ revgeocode(c(.x, .y), output = "address")))
unnest(singer_loc)
```
I tried to use `possibly()` to avoid generating side effects. The result using `singer_locations` is same, but when the input is not proper, this function returns "Not proper inputs". Thus, it is possible to use observations without longitude, latitude information (Output would be "Not proper inputs"). I don't need to use `filter` to remove all the observations with `NA`.
```{r}

#A new function `revgeocode2()` using `possibly()`
revgeocode2 <- possibly(~ revgeocode(c(.x, .y), output = "address"), 
                        otherwise = "*****Not proper inputs*****")
#Same result as using `revgeocode()`
singer_loc2 <- singer_locations %>% 
  filter(!is.na(longitude|latitude)) %>% 
  head() %>% 
  mutate(loc = map2(longitude, latitude,
                    ~ revgeocode2(.x, .y)))
unnest(singer_loc2)


##When the observations with NA are included, still it is possible to use the function `revgeocode2()`. The output for those observations are "Not proper inputs"
singer_loc3 <- singer_locations %>% 
  head() %>% 
  mutate(loc= map2(longitude, latitude,
                   ~ revgeocode2(.x, .y)))
unnest(singer_loc3)

#When the input is c(NA, NA)
revgeocode2(NA, NA)
```


2. Try to check wether the place in `city` corresponds to the information you retrieved.   
I tried to use `output = "more"` in `revgeocode()` to extract each element in address information. Information in `singer_loc4$city` contains only `city`, only `state`, or `city` with `state`. I tried to compare both. But the problem I got in was when I extract each element in address information using `output = "more"`, it returns full name of states. What I wanted was state abbreviations for comparing with `singer_loc4$city` (when `output = "address"` is used, the state names are in abbreviations.). I fixed this with `state.abb[grep(.x, state.name)]`. Then, the place in `city` corresponds to the information I retrieved.
```{r}
singer_loc4 <- singer_locations %>% 
  filter(!is.na(longitude|latitude)) %>% 
  head() %>% 
  mutate(loc = map2(longitude, 
                    latitude, 
                    ~ revgeocode(c(.x, .y), output = "more"))) %>% 
  unnest()

#Mutating state abbreviations for administrative_area_level_1
singer_loc4 <- singer_loc4 %>% 
  mutate(admin_lv1_abb = map(administrative_area_level_1,
                             ~ state.abb[grep(.x, state.name)])) %>% 
  unnest()

#Mutating city_state (city, state)
singer_loc4 <- singer_loc4 %>% 
  mutate(city_state = paste(locality, admin_lv1_abb, sep = ", "))

# Comparing `city` from the data `singer_locations` and information extracted from longitude and latitude
singer_loc4 <- singer_loc4 %>% 
  mutate(Correspondence = ifelse((city == locality)|
                                   (city == administrative_area_level_1)|
                                   (city == city_state), "o", "xxx"))

singer_loc4 
# %>% 
#  select(-street_number, -route, -neighborhood, -administrative_area_level_2, 
#         -country, -postal_code, -political, -administrative_area_level_3)
```

3. If you still have time, you can go visual: give a look to the library [`leaflet`](https://rstudio.github.io/leaflet) and plot some information about the bands. A snippet of code is provided below.  
```{r}
#install.packages('leaflet')
library(leaflet)
singer_locations %>%  
  leaflet()  %>%   
  addTiles() %>%  
  addCircles(popup = ~artist_name)
```


