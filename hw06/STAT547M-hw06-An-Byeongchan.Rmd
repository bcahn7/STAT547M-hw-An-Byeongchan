---
title: "STAT547M-hw06-An-Byeongchan"
output: 
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Pick (at least) two of the six (numbered) topics below and do one of the exercise prompts listed, or something comparable using your dataset of choice.

### 1. Character data

Read and work the exercises in the [Strings
chapter](http://r4ds.had.co.nz/strings.html) or R for Data Science.
```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(stringr))

#Practice
x <- "\"" # or '"'
writeLines(x)
x <- '\' \\' # or "'"
writeLines(x)
x <- "\u00b5"
writeLines(x)

#Combining strings 
str_c("Hi ", "How ", "are ", "you")
str_c("x", "y", sep = ", ")

#str_c() is vectorized. 
str_c("prefix-", c("a", "b", "c"), "-suffix")

#Load some tweets from the official Donald Trump account.
load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))
tweets <- trump_tweets_df$text
tweets %>% head() %>% strtrim(70)

regex <- "badly|crazy|weak|spent|strong|dumb|joke|guns|funny|dead"

tweets <- tweets[c(1, 2, 5, 6, 198, 347, 919)]
tweets %>% strtrim(70)

matches <- gregexpr(regex, tweets)
matches
str(matches)
#View(matches)
lengths(matches) #lengths
map_int(matches, length)

## Exercise: Get a list of the match lengths
#Extracting attributes named `match.length()` from each element of the list `matches`
map(matches, function(x) attr(x, which = "match.length"))
map(matches, ~attr(.x, which = "match.length"))
#map(matches, ~ attr(.x, which = "match.length"))

## Exercise: Count the number of Trump Android words in each tweets.

# lengths() is for the lengths of the elements of `matches`
# This cannot distinguish between 0 and 1.
lengths(matches)
# Thus, apply `map_int()`!!
map_int(matches, function(x) sum(x>0))

# Removing attr for less clutter
# Using `as.vector()` to strip attributes.
match_vec <- map(matches, as.vector)
match_vec

# Extracting exact words mentioned on the tweet
# What words are mentioned in tweet #7??
tweet_7 <- tweets[7]
match_first <- map(matches, as.vector)
match_length <- map(matches, function(x) attr(x, which = "match.length"))


(t_first <- match_first[[7]])
(t_length <- match_length[[7]])
(t_last <- match_first[[7]] + match_length[[7]] - 1)

substring(tweet_7, t_first, t_last)

#Expand it to all the elements on the list.
match_last  <- map2(match_first, match_length, ~.x + .y -1) # 2 inputs

pmap(.l= list(text = tweets, first = match_first, last = match_last), substring) # 3 inputs

#another way to do this
mdf <- tibble(
  text = tweets,
  first = match_first,
  last = match_last
)
pmap(mdf, substring)

```


## Steps to extract the words in a text
Pseudo-code
> text <- an element of a text
first <- an element of match_first
last <- an element of match_last
substring(text, first, last)


## Application to Finance
I tried to apply this skill to analyze the minutes of the FOMC (Federal Open Market Committee) meetings.   
  
I downloaded the data from https://stanford.edu/~rezab/useful/fomc_minutes.html 
  
Downloaded: 3 minutes (20010627, 20061025, 20071211)
Bag of words: inflation, investment, markets, economy, monetary, unemployment  

This is basic and introductory textual analysis to see if there is a change in the number of words mentioned during different periods (IT bubble, 2008 Financial Crisis)
```{r}
#m200106 <- scan("Data/20010627.txt", what="character", sep = NULL)

m200106 <- read_file("Data/20010627.txt")
m200610 <- read_file("Data/20061025.txt")
m200712 <- read_file("Data/20071211.txt")


bow <- "inflation|investment|markets|economy|monetary|unemployment"
collection  <-  list(m200106, m200610, m200712)
str(collection)


wrd_mtnd <- tibble(text = collection,
       first = gregexpr(bow, collection)) %>% 
  mutate (match_length = map(first, ~attr(.x, which = "match.length")),
          last = map2(first, match_length, ~ .x + .y -1)) %>% 
  select(-match_length) %>% 
  pmap(substring)
map(wrd_mtnd, ~table(.x)) 
```


### 2. Writing functions

Pick one:

  * Write one (or more) functions that do something useful to pieces of the
Gapminder or Singer data. It is logical to think about computing on the mini-data frames
corresponding to the data for each specific country, location, year, band, album, ... This would pair well with
the prompt below about working with a nested data frame, as you could apply your
function there.
    - Make it something you can't easily do with built-in functions.
Make it something that's not trivial to do with the simple `dplyr` verbs. The
linear regression function [presented
here](block012_function-regress-lifeexp-on-year.html) is a good starting point.
You could generalize that to do quadratic regression (include a squared term) or
use robust regression, using `MASS::rlm()` or `robustbase::lmrob()`.
```{r}
suppressPackageStartupMessages(library(gapminder))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(MASS))

names(gapminder)
#Cross-sectional data (lifeExp vs gdpPercap) in 2007
gap_2007 <- gapminder %>% 
  filter(year==2007) 
gap_2007 %>% 
  ggplot(aes(x=lifeExp, y=gdpPercap)) +
  geom_point() +
  geom_smooth(method = "lm", se=FALSE, color = "blue") +
  geom_smooth(method = "lm", se=FALSE, formula = y~poly(x,2), color= "green") +
  geom_smooth(method = "rlm", se=FALSE, color= "red") +
  geom_smooth(method = "auto", se=FALSE, color = "purple")

lm(gdpPercap ~ lifeExp, gap_2007)
gap_2007_temp <- gap_2007 %>% 
  mutate(lifeExp2=  lifeExp^2)
lm(gdpPercap ~ lifeExp + lifeExp2, gap_2007_temp)
rlm(gdpPercap ~ lifeExp, gap_2007)

Reg_life_gdp<- function(mode, year){
  gap_year <- gapminder %>% 
    filter(year == year) %>% 
    mutate(lifeExp2 = lifeExp^2)
  if(mode=="lm"){
    the_fit <- lm(gdpPercap ~ lifeExp, gap_year)
    setNames(coef(the_fit), c("intercept", "slope"))
  }
  else if(mode=="qm"){
    the_fit <- lm(gdpPercap ~ lifeExp + lifeExp2, gap_year)
    setNames(coef(the_fit), c("intercept", "slope1", "slope2"))
  }
  else if(mode=="rlm"){
    the_fit <- rlm(gdpPercap ~ lifeExp, gap_year)
    setNames(coef(the_fit), c("intercept", "slope1"))    
  }
}

Reg_life_gdp("lm", 2002)
Reg_life_gdp("rlm", 2007)
```


  * If you plan to complete the homework where we build an R package, write a couple of experimental functions exploring some functionality that is useful to you in real life and that might form the basis of your personal package.

### 3. Work with the candy data

In 2015, we explored a dataset based on a Halloween candy survey (but it included many other odd and interesting questions). Work on something from [this homework from 2015](hw07_2015_data-wrangling-candy.html). It is good practice on basic
data ingest, exploration, character data cleanup, and wrangling.

### 4. Work with the `singer` data

The `singer_location` dataframe in the `singer` package contains geographical information stored in two different formats: 1. as a (dirty!) variable named `city`; 2. as a latitude / longitude pair (stored in `latitude`, `longitude` respectively). The function `revgeocode` from the `ggmap` library allows you to retrieve some information for a pair (vector) of longitude, latitude (warning: notice the order in which you need to pass lat and long). Read its manual page.
```{r}
suppressPackageStartupMessages(library(singer))
suppressPackageStartupMessages(library(tidyverse))
#install.packages('ggmap')
suppressPackageStartupMessages(library(ggmap))
#View(singer_locations)
```

1. Use `purrr` to map latitude and longitude into human readable information on the band's origin places. Notice that `revgeocode(... , output = "more")` outputs a dataframe, while `revgeocode(... , output = "address")` returns a string: you have the option of dealing with nested dataframes.  
You will need to pay attention to two things:  
    *  Not all of the track have a latitude and longitude: what can we do with the missing information? (_filtering_, ...)
    *  Not all of the time we make a research through `revgeocode()` we get a result. What can we do to avoid those errors to bite us? (look at _possibly()_ in `purrr`...)
```{r}
View(singer_locations)
#singer_locations %>% 
#  mutate(test = paste("latitude, longitude, collapse = " ,"))
```


2. Try to check wether the place in `city` corresponds to the information you retrieved.

3. If you still have time, you can go visual: give a look to the library [`leaflet`](https://rstudio.github.io/leaflet) and plot some information about the bands. A snippet of code is provided below.  
```r
singer_locations %>%  
  leaflet()  %>%   
  addTiles() %>%  
  addCircles(popup = ~artist_name)
```

### 5. Work with a list

Work through and write up a lesson from the [purrr
tutorial](https://jennybc.github.io/purrr-tutorial/index.html):

  * [Trump Android
Tweets](https://jennybc.github.io/purrr-tutorial/ls08_trump-tweets.html)
  * [Simplifying data from a list of GitHub
users](https://jennybc.github.io/purrr-tutorial/ls02_map-extraction-advanced.html)

### 6. Work with a nested data frame

Create a nested data frame and map a function over the list column holding the
nested data. Use list extraction or other functions to pull interesting
information out of these results and work your way back to a simple data frame
you can visualize and explore.

Here's a fully developed prompt for Gapminder:

  * See the [split-apply-combine lesson from
class](block024_group-nest-split-map.html)
  * Nest the data by country (and continent).
  * Fit a model of life expectancy against year. Possibly quadratic,
possibly robust (see above prompt re: function writing).
  * Use functions for working with fitted models or the [broom
package](https://github.com/tidyverse/broom) to get information out of your
linear models.
  * Use the usual dplyr, tidyr, and ggplot2 workflows to explore,
e.g., the estimated cofficients.

Inspiration for the modelling and downstream inspiration

  * Find countries with interesting stories. - Sudden, substantial departures from the temporal trend is interesting. How could you operationalize this notion of "interesting"?
  * Use the residuals to detect countries where your model is a
terrible fit. Examples: Are there are 1 or more freakishly large residuals, in
an absolute sense or relative to some estimate of background variability? Are
there strong patterns in the sign of the residuals? E.g., all pos, then all neg,
then pos again.
  * Fit a regression using ordinary least squares and a robust
technique. Determine the difference in estimated parameters under the two
approaches. If it is large, consider that country "interesting".
  * Compare a linear and quadratic fit

